<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Points-to-3D: Structure-Aware 3D Generation with Point Cloud Priors. A diffusion-based framework leveraging point cloud priors for geometry-controllable 3D generation.">
  <meta name="keywords" content="Points-to-3D, 3D Generation, Point Cloud, TRELLIS, Diffusion Models">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Points-to-3D: Structure-Aware 3D Generation with Point Cloud Priors</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="https://www.adelaide.edu.au/aiml/">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a>
    </div>

  </div>
</nav>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Points-to-3D: Structure-Aware 3D Generation with Point Cloud Priors</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://www.linkedin.com/in/jiatong-xia/?locale=en_US">Jiatong Xia</a><sup>*</sup>,</span>
            <span class="author-block">
              <a href="https://zichengduan.github.io/">Zicheng Duan</a><sup>*</sup>,</span>
            <span class="author-block">
              <a href="https://cs.adelaide.edu.au/~hengel/">Anton van den Hengel</a>,
            </span>
            <span class="author-block">
              <a href="https://researchers.adelaide.edu.au/profile/lingqiao.liu">Lingqiao Liu</a><sup>†</sup>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block">Australian Institute for Machine Learning, University of Adelaide</span>
          </div>

          <div class="is-size-6 publication-authors" style="margin-top: 10px;">
            <span class="author-block"><sup>*</sup>Equal contribution</span>
            <span class="author-block"><sup>†</sup>Corresponding author</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="./CVPR_2026.pdf"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href="./CVPR_2026_supp.pdf"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Supplementary</span>
                </a>
              </span>
              <span class="link-block">
                <a href="#"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv (Coming Soon)</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="#"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code (Coming Soon)</span>
                  </a>
              </span>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <img src="./static/images/teaser.png" alt="Points-to-3D Teaser" style="width: 100%; height: auto;">
      <h2 class="subtitle has-text-centered" style="margin-top: 20px;">
        We introduce explicit 3D point cloud priors into 3D generation framework. Given a pre-existing point cloud or
        a feed-forward point cloud prediction from image input, our model generates high-quality 3D assets that
        faithfully preserve the observed structure while plausibly completing unobserved regions with coherent geometry.
      </h2>
    </div>
  </div>
</section>


<section class="hero is-light is-small">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3 has-text-centered">Method Overview</h2>
      <div class="content has-text-centered">
        <img src="./static/images/pipeline.png" alt="Method Overview" style="max-width: 100%; height: auto;">
        <p class="has-text-justified" style="margin-top: 20px;">
          <strong>Overall framework.</strong> Given point cloud priors—either pre-existing or predicted by VGGT from input image—we first
          voxelize and VAE-encode it to obtain an SS latent, where the empty regions are filled with random noise and concatenated with an
          extracted mask to form the input paradigm for our model. During training, we optimize our inpainting flow transformer via conditional
          flow matching loss. During inference, we employ a two-stage sampling procedure: (1) structural inpainting with s steps to inpaint
          the global structure, and (2) boundary refinement with remaining (t-s) steps to refine the inpainting boundaries.
        </p>
      </div>
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Recent progress in 3D generation has been driven largely by models conditioned on images or text,
            while readily available 3D priors are still underused. In many real-world scenarios, visible-region
            point clouds are easy to obtain—from active sensors such as LiDAR or from feed-forward predictors
            like VGGT—offering explicit geometric constraints that current methods fail to exploit.
          </p>
          <p>
            In this work, we introduce <span class="dnerf">Points-to-3D</span>, a diffusion-based framework that
            leverages point cloud priors for geometry-controllable 3D asset and scene generation. Built on the latent
            3D diffusion model TRELLIS, Points-to-3D first replaces pure-noise sparse structure latent initialization
            with a point cloud priors tailored input formulation. A structure inpainting network, trained within the
            TRELLIS framework on task-specific data designed to learn global structural inpainting, is then used for
            inference with a staged sampling strategy (structural inpainting followed by boundary refinement),
            completing the global geometry while preserving the visible regions of the input priors.
          </p>
          <p>
            In practice, Points-to-3D can take either accurate point-cloud priors or VGGT-estimated point clouds from
            single images as input. Experiments on both single-object and multi-object scenarios consistently demonstrate
            superior performance over state-of-the-art baselines in terms of rendering quality and geometric fidelity,
            highlighting the effectiveness of explicitly embedding point-cloud priors for achieving more accurate and
            structurally controllable 3D generation.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->

    <!-- Training Data -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Training Data Construction</h2>
        <div class="content has-text-centered">
          <img src="./static/images/data0.png" alt="Training Data Processing" style="max-width: 82%; height: auto;">
          <p class="has-text-justified" style="margin-top: 20px;">
            <strong>Training data processing.</strong> We preserve the visible portion of the complete point cloud and convert it into training inputs.
            We render depth maps from multiple viewpoints and extract visible point clouds using depth consistency checks, then encode them into
            sparse structure latents paired with ground-truth complete structures for training.
          </p>
        </div>
      </div>
    </div>
    <!--/ Training Data. -->

    <!-- Point Cloud Examples -->
    <div class="columns is-centered has-text-centered" style="margin-top: 40px;">
      <div class="column is-full-width">
        <h2 class="title is-3">Point Cloud Priors</h2>
        <div class="content has-text-centered">
          <img src="./static/images/pc_demo.png" alt="Point Cloud Examples" style="max-width: 100%; height: auto;">
          <p class="has-text-justified" style="margin-top: 20px;">
            <strong>Input point cloud priors examples.</strong> We support two types of point cloud priors: (1) <strong>Sampled Point Cloud</strong> -
            partial point clouds directly captured by hardware sensors (e.g., LiDAR on an iPhone), and (2) <strong>VGGT Estimated</strong> -
            point clouds estimated from input images via feed-forward point-map prediction. Both types impose reliable geometric constraints
            that steer our model toward controllable and faithful 3D generation.
          </p>
        </div>
      </div>
    </div>
    <!--/ Point Cloud Examples. -->
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">

    <!-- Results Title -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-full-width">
        <h2 class="title is-2">Experimental Results</h2>
      </div>
    </div>

    <!-- Single-Object Results -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Single-Object Generation on Toys4K</h2>

        <!-- Quantitative Results Table -->
        <div class="content" style="margin-bottom: 30px;">
          <div style="overflow-x: auto;">
            <table class="table is-bordered is-striped is-narrow is-hoverable" style="margin: 0 auto; font-size: 0.9em;">
              <thead>
                <tr>
                  <th rowspan="2">Method</th>
                  <th colspan="4" style="text-align: center; border-bottom: 2px solid #dbdbdb;">Rendering</th>
                  <th colspan="4" style="text-align: center; border-bottom: 2px solid #dbdbdb;">Geometry</th>
                </tr>
                <tr>
                  <th>PSNR↑</th>
                  <th>SSIM(%)↑</th>
                  <th>LPIPS↓</th>
                  <th>DINO(%)↓</th>
                  <th>CD↓</th>
                  <th>F-Score↑</th>
                  <th>PSNR-N↑</th>
                  <th>LPIPS-N↓</th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>GaussianAnything</td>
                  <td>20.08</td>
                  <td>89.31</td>
                  <td>0.183</td>
                  <td>26.74</td>
                  <td>0.084</td>
                  <td>0.513</td>
                  <td>20.99</td>
                  <td>0.199</td>
                </tr>
                <tr>
                  <td>Real3D</td>
                  <td>19.55</td>
                  <td>90.65</td>
                  <td>0.169</td>
                  <td>27.65</td>
                  <td>0.065</td>
                  <td>0.574</td>
                  <td>21.31</td>
                  <td>0.178</td>
                </tr>
                <tr>
                  <td>LGM</td>
                  <td>20.55</td>
                  <td>89.98</td>
                  <td>0.181</td>
                  <td>23.45</td>
                  <td>0.075</td>
                  <td>0.487</td>
                  <td>20.04</td>
                  <td>0.202</td>
                </tr>
                <tr>
                  <td>VoxHammer (3D Inversion)</td>
                  <td>20.51</td>
                  <td>90.01</td>
                  <td>0.123</td>
                  <td>15.10</td>
                  <td>0.046</td>
                  <td>0.724</td>
                  <td>20.28</td>
                  <td>0.158</td>
                </tr>
                <tr style="background-color: #fffacd;">
                  <td><strong>TRELLIS</strong></td>
                  <td>21.94</td>
                  <td>91.46</td>
                  <td>0.105</td>
                  <td>7.82</td>
                  <td>0.034</td>
                  <td>0.832</td>
                  <td>23.81</td>
                  <td>0.105</td>
                </tr>
                <tr style="background-color: #ffdab9;">
                  <td><strong>Points-to-3D (Ours-VGGT Esti.)</strong></td>
                  <td>22.55</td>
                  <td>92.09</td>
                  <td>0.088</td>
                  <td>7.37</td>
                  <td>0.024</td>
                  <td>0.881</td>
                  <td>24.53</td>
                  <td>0.085</td>
                </tr>
                <tr style="background-color: #ffcccc;">
                  <td><strong>Points-to-3D (Ours-P.C.Priors)</strong></td>
                  <td><strong>22.91</strong></td>
                  <td><strong>92.83</strong></td>
                  <td><strong>0.070</strong></td>
                  <td><strong>7.29</strong></td>
                  <td><strong>0.013</strong></td>
                  <td><strong>0.964</strong></td>
                  <td><strong>27.10</strong></td>
                  <td><strong>0.053</strong></td>
                </tr>
              </tbody>
            </table>
          </div>
          <p class="has-text-centered" style="margin-top: 10px; font-size: 0.9em; color: #666;">
            <strong>Table 1.</strong> Comparison on single-object generation on Toys4K dataset. We showcase the performance of our method
            in two scenarios: one where explicit point cloud priors are provided, and another where point clouds are inferred from condition
            images using VGGT.
          </p>
        </div>

        <!-- Qualitative Results -->
        <div class="content">
          <img src="./static/images/toys4k0.png" alt="Toys4K Results" style="width: 100%; height: auto;">
          <p class="has-text-justified" style="margin-top: 15px;">
            <strong>Single-object generation on Toys4K.</strong> For the explicit point cloud priors results, we use point clouds extracted strictly
            from the visible region of input images, whereas the "VGGT-estimated" results use point clouds inferred from the condition images by VGGT.
            Our method consistently outperforms existing approaches across all evaluation metrics. With point cloud priors, we achieve an F-score of 0.964,
            demonstrating that our approach produces geometry that closely approximates the ground-truth structure. Normal maps further highlight the
            superior geometric quality achieved by our approach.
          </p>
        </div>
      </div>
    </div>

    <!-- Multi-Object Results -->
    <div class="columns is-centered" style="margin-top: 40px;">
      <div class="column is-full-width">
        <h2 class="title is-3">Multi-Object Generation on 3D-FRONT</h2>

        <!-- Quantitative Results Table -->
        <div class="content" style="margin-bottom: 30px;">
          <div style="overflow-x: auto;">
            <table class="table is-bordered is-striped is-narrow is-hoverable" style="margin: 0 auto; font-size: 0.9em;">
              <thead>
                <tr>
                  <th rowspan="2">Method</th>
                  <th colspan="4" style="text-align: center; border-bottom: 2px solid #dbdbdb;">Rendering</th>
                  <th colspan="4" style="text-align: center; border-bottom: 2px solid #dbdbdb;">Geometry</th>
                </tr>
                <tr>
                  <th>PSNR↑</th>
                  <th>SSIM(%)↑</th>
                  <th>LPIPS↓</th>
                  <th>DINO(%)↓</th>
                  <th>CD↓</th>
                  <th>F-Score↑</th>
                  <th>PSNR-N↑</th>
                  <th>LPIPS-N↓</th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>TRELLIS</td>
                  <td>18.21</td>
                  <td>83.12</td>
                  <td>0.239</td>
                  <td>12.33</td>
                  <td>0.094</td>
                  <td>0.478</td>
                  <td>18.76</td>
                  <td>0.258</td>
                </tr>
                <tr>
                  <td>VoxHammer (3D Inversion)</td>
                  <td>19.29</td>
                  <td>84.70</td>
                  <td>0.179</td>
                  <td>18.41</td>
                  <td>0.051</td>
                  <td>0.686</td>
                  <td>20.43</td>
                  <td>0.181</td>
                </tr>
                <tr>
                  <td>SceneGen</td>
                  <td>18.32</td>
                  <td>83.35</td>
                  <td>0.231</td>
                  <td>14.43</td>
                  <td>0.086</td>
                  <td>0.485</td>
                  <td>19.08</td>
                  <td>0.229</td>
                </tr>
                <tr style="background-color: #fffacd;">
                  <td><strong>MIDI</strong></td>
                  <td>19.23</td>
                  <td>85.59</td>
                  <td>0.166</td>
                  <td>14.25</td>
                  <td>0.075</td>
                  <td>0.513</td>
                  <td>20.82</td>
                  <td>0.164</td>
                </tr>
                <tr style="background-color: #ffdab9;">
                  <td><strong>Points-to-3D (Ours-VGGT Esti.)</strong></td>
                  <td>20.52</td>
                  <td>86.51</td>
                  <td>0.152</td>
                  <td>8.90</td>
                  <td>0.040</td>
                  <td>0.743</td>
                  <td>20.97</td>
                  <td>0.160</td>
                </tr>
                <tr style="background-color: #ffcccc;">
                  <td><strong>Points-to-3D (Ours-P.C.Priors)</strong></td>
                  <td><strong>21.63</strong></td>
                  <td><strong>87.73</strong></td>
                  <td><strong>0.124</strong></td>
                  <td><strong>8.29</strong></td>
                  <td><strong>0.025</strong></td>
                  <td><strong>0.886</strong></td>
                  <td><strong>22.38</strong></td>
                  <td><strong>0.124</strong></td>
                </tr>
              </tbody>
            </table>
          </div>
          <p class="has-text-centered" style="margin-top: 10px; font-size: 0.9em; color: #666;">
            <strong>Table 2.</strong> Comparison on multi-object generation on 3D-FRONT dataset. Points-to-3D consistently outperforms
            state-of-the-art multi-object generation methods across all evaluation metrics.
          </p>
        </div>

        <!-- Qualitative Results -->
        <div class="content">
          <img src="./static/images/3dfront0.png" alt="3D-FRONT Results" style="width: 100%; height: auto;">
          <p class="has-text-justified" style="margin-top: 15px;">
            <strong>Multi-object generation on 3D-FRONT.</strong> Our method achieves significant improvements across all evaluation metrics
            compared to other methods in complex multi-object scenarios. Unlike MIDI or SceneGen which implicitly utilize spatial information,
            our framework explicitly incorporates geometric priors within the architecture, enabling more direct and effective control over
            3D geometry. The rendered images and normal maps demonstrate that our results better align with the ground-truth scene geometry
            and visual appearance.
          </p>
        </div>
      </div>
    </div>

    <!-- Visible Region Performance -->
    <div class="columns is-centered" style="margin-top: 40px;">
      <div class="column is-full-width">
        <h2 class="title is-3">Visible Region Performance</h2>

        <!-- Table -->
        <div class="content" style="margin-bottom: 30px;">
          <div style="overflow-x: auto;">
            <table class="table is-bordered is-striped is-narrow is-hoverable" style="margin: 0 auto; font-size: 0.9em; max-width: 600px;">
              <thead>
                <tr>
                  <th>Methods</th>
                  <th>CD↓</th>
                  <th>F-Score↑</th>
                  <th>PSNR-N↑</th>
                  <th>LPIPS-N↓</th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>TRELLIS-O.</td>
                  <td>0.034</td>
                  <td>0.832</td>
                  <td>23.81</td>
                  <td>0.105</td>
                </tr>
                <tr>
                  <td>TRELLIS-V.</td>
                  <td>0.032</td>
                  <td>0.854</td>
                  <td>24.77</td>
                  <td>0.093</td>
                </tr>
                <tr style="background-color: #ffcccc;">
                  <td><strong>Points-to-3D-O.</strong></td>
                  <td><strong>0.013</strong></td>
                  <td><strong>0.964</strong></td>
                  <td><strong>27.10</strong></td>
                  <td><strong>0.053</strong></td>
                </tr>
                <tr style="background-color: #ffcccc;">
                  <td><strong>Points-to-3D-V.</strong></td>
                  <td><strong>0.007</strong></td>
                  <td><strong>0.998</strong></td>
                  <td><strong>29.00</strong></td>
                  <td><strong>0.036</strong></td>
                </tr>
              </tbody>
            </table>
          </div>
          <p class="has-text-centered" style="margin-top: 10px; font-size: 0.9em; color: #666;">
            <strong>Table 3.</strong> Comparison on visible and overall geometry results on Toys4K. For each method, the upper row (O.) shows
            the overall results, while the lower row (V.) shows the visible region results. Our method achieves F-score of 0.998 in visible regions,
            demonstrating strong alignment with ground truth structure.
          </p>
        </div>
      </div>
    </div>

    <!-- Ablation Study -->
    <div class="columns is-centered" style="margin-top: 40px;">
      <div class="column is-full-width">
        <h2 class="title is-3">Ablation Study: Staged Sampling Strategy</h2>

        <!-- Table -->
        <div class="content" style="margin-bottom: 30px;">
          <div style="overflow-x: auto;">
            <table class="table is-bordered is-striped is-narrow is-hoverable" style="margin: 0 auto; font-size: 0.9em; max-width: 600px;">
              <thead>
                <tr>
                  <th>Inp.</th>
                  <th>Ref.</th>
                  <th>CD↓</th>
                  <th>F-Score↑</th>
                  <th>PSNR-N↑</th>
                  <th>LPIPS-N↓</th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>50</td>
                  <td>0</td>
                  <td>0.014</td>
                  <td>0.960</td>
                  <td>25.88</td>
                  <td>0.065</td>
                </tr>
                <tr>
                  <td>40</td>
                  <td>10</td>
                  <td>0.013</td>
                  <td>0.962</td>
                  <td>26.49</td>
                  <td>0.059</td>
                </tr>
                <tr>
                  <td>30</td>
                  <td>20</td>
                  <td>0.013</td>
                  <td>0.963</td>
                  <td>26.89</td>
                  <td>0.056</td>
                </tr>
                <tr style="background-color: #ffcccc;">
                  <td><strong>25</strong></td>
                  <td><strong>25</strong></td>
                  <td><strong>0.013</strong></td>
                  <td><strong>0.963</strong></td>
                  <td><strong>27.10</strong></td>
                  <td><strong>0.053</strong></td>
                </tr>
                <tr>
                  <td>20</td>
                  <td>30</td>
                  <td>0.013</td>
                  <td>0.962</td>
                  <td>27.03</td>
                  <td>0.055</td>
                </tr>
                <tr>
                  <td>10</td>
                  <td>40</td>
                  <td>0.014</td>
                  <td>0.961</td>
                  <td>26.72</td>
                  <td>0.061</td>
                </tr>
              </tbody>
            </table>
          </div>
          <p class="has-text-centered" style="margin-top: 10px; font-size: 0.9em; color: #666;">
            <strong>Table 4.</strong> Ablation study on the number of inpainting steps (Inp.) and refinement steps (Ref.) in our sampling strategy.
            The best performance is achieved with 25 inpainting steps and 25 refinement steps.
          </p>
        </div>

        <!-- Visualization -->
        <div class="content">
          <img src="./static/images/abla.png" alt="Ablation Study" style="width: 100%; height: auto;">
          <p class="has-text-justified" style="margin-top: 15px;">
            <strong>Ablation study.</strong> Allocating the full sampling to inpainting (Inp.) results in geometric "holes" along the inpainting edge.
            By setting the sampling schedule to 25 inpainting steps followed by 25 refinement steps, the geometric metrics reach their best performance,
            and the previously observed "holes" are effectively eliminated, yielding the overall best generation results.
          </p>
        </div>
      </div>
    </div>

    <!-- Real-world and Text-to-3D -->
    <div class="columns is-centered" style="margin-top: 40px;">
      <div class="column">
        <h2 class="title is-3">Real-world Examples</h2>
        <div class="content">
          <img src="./static/images/real-world.png" alt="Real-world Examples" style="width: 100%; height: auto;">
          <p class="has-text-justified" style="margin-top: 15px;">
            <strong>Real-world examples on Pix3D.</strong> Our approach maintains robust performance on real image inputs,
            producing geometry that aligns more faithfully with the input images compared to the baseline method.
          </p>
        </div>
      </div>

      <div class="column">
        <h2 class="title is-3">Text-to-3D Generation</h2>
        <div class="content">
          <img src="./static/images/text.png" alt="Text-to-3D Results" style="width: 100%; height: auto;">
          <p class="has-text-justified" style="margin-top: 15px;">
            <strong>Text-to-3D generation on Toys4K.</strong> Our method successfully generates geometries that are semantically
            consistent with the input prompts and structurally well-controlled by the given point cloud priors.
          </p>
        </div>
      </div>
    </div>

    <!-- Multi-view Input -->
    <div class="columns is-centered" style="margin-top: 40px;">
      <div class="column is-full-width">
        <h2 class="title is-3">Multi-View Input Generation</h2>
        <div class="content">
          <img src="./static/images/3views.png" alt="Multi-view Results" style="width: 100%; height: auto;">
          <p class="has-text-justified" style="margin-top: 15px;">
            <strong>Generation results with 3 input views on Toys4K.</strong> Our flow-based model can directly incorporate multi-view reference
            images at different denoising steps. For VGGT-estimated point clouds, multi-view inputs produce more accurate predictions. With accurate
            point cloud priors extracted from three views, our method produces reconstructions that are very close to the ground truth, demonstrating
            the robustness and effectiveness across different numbers of input images.
          </p>
        </div>
      </div>
    </div>

    <!-- More Results -->
    <div class="columns is-centered" style="margin-top: 40px;">
      <div class="column is-full-width">
        <h2 class="title is-3">Additional Results</h2>
        <div class="content">
          <img src="./static/images/more.png" alt="More Results" style="width: 100%; height: auto;">
          <p class="has-text-justified" style="margin-top: 15px;">
            <strong>More generation examples.</strong> Our method demonstrates consistent high-quality results across diverse object categories
            and scene types, highlighting the effectiveness of explicitly embedding point-cloud priors for achieving accurate and structurally
            controllable 3D generation.
          </p>
        </div>
      </div>
    </div>

  </div>
</section>


<!-- Key Contributions Section -->
<section class="section" style="background-color: #f5f5f5;">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3 has-text-centered">Key Contributions</h2>
        <div class="content has-text-justified">
          <ul style="font-size: 1.1em; line-height: 1.8;">
            <li><strong>Explicit 3D Prior Integration:</strong> Unlike existing methods that condition solely on images or text, we explicitly
            incorporate point cloud priors into the latent space, enabling precise geometric control over generated 3D assets.</li>

            <li><strong>Inpainting-based Framework:</strong> We formulate 3D generation as a latent inpainting problem, preserving observed
            structures while plausibly completing unobserved regions guided by visible-region cues.</li>

            <li><strong>Staged Sampling Strategy:</strong> Our two-stage approach (structural inpainting + boundary refinement) effectively
            addresses the "holes" problem at inpainting boundaries, ensuring both global consistency and local precision.</li>

            <li><strong>Flexible Input Support:</strong> The method supports both hardware-sensed point clouds (e.g., LiDAR) and
            feed-forward predicted point clouds (e.g., VGGT), enabling deployment in various real-world scenarios.</li>

            <li><strong>Superior Performance:</strong> Experiments demonstrate F-scores up to 0.998 in visible regions and 0.964 overall,
            significantly outperforming state-of-the-art baselines in both rendering quality and geometric fidelity.</li>
          </ul>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- <section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@inproceedings{xia2026points2-3d,
  author    = {Xia, Jiatong and Duan, Zicheng and van den Hengel, Anton and Liu, Lingqiao},
  title     = {Points-to-3D: Structure-Aware 3D Generation with Point Cloud Priors},
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  year      = {2026},
}</code></pre>
  </div>
</section> -->


<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link"
         href="./CVPR_2026.pdf">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="#" class="external-link">
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content has-text-centered">
          <p>
            © 2026 Australian Institute for Machine Learning, University of Adelaide
          </p>
          <p>
            This website is adapted from <a href="https://github.com/nerfies/nerfies.github.io">Nerfies</a>,
            licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
